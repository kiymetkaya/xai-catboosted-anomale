{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Hjc3iIihKLn-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from dgl.data import DGLDataset\n",
    "import dgl\n",
    "import time\n",
    "import networkx as nx\n",
    "import category_encoders as ce\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import tqdm\n",
    "import math\n",
    "\n",
    "from typing import *\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kvz2rBKN32yd"
   },
   "source": [
    "## Loading Graphs\n",
    "* Multigraph with\n",
    "    - Edge features\n",
    "        - h : A list of features in data\n",
    "        - Label (0-1)\n",
    "        - Attack\n",
    "    - Node features\n",
    "        - {1,.., 1} same length as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"test.graph\"\n",
    "# Use dgl.save_graphs to save the graph to the specified file\n",
    "##dgl.save_graphs(file_path, [test_g])\n",
    "# Use dgl.load_graphs to load the graph from the file\n",
    "test_g, _ = dgl.load_graphs(file_path)\n",
    "# The loaded_graphs variable now contains the loaded DGLGraph(s)\n",
    "test_g = test_g[0]  # Assuming you saved a single graph\n",
    "\n",
    "# Specify the path to the saved graph file\n",
    "file_path = \"train.graph\"\n",
    "# Use dgl.save_graphs to save the graph to the specified file\n",
    "##dgl.save_graphs(file_path, [train_g])\n",
    "# Use dgl.load_graphs to load the graph from the file\n",
    "train_g, _ = dgl.load_graphs(file_path)\n",
    "# The loaded_graphs variable now contains the loaded DGLGraph(s)\n",
    "train_g = train_g[0]  # Assuming you saved a single graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "lab_enc = joblib.load('gnn_label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIbmcMJe32yd"
   },
   "source": [
    "# Self-Supervised Learning\n",
    "### E-GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "PUV6DgJ9QRaP"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import tqdm\n",
    "import gc\n",
    "\n",
    "class SAGELayer(nn.Module):\n",
    "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
    "      super(SAGELayer, self).__init__()\n",
    "      self.W_apply = nn.Linear(ndim_in + edims , ndim_out)\n",
    "      self.activation = F.relu\n",
    "      self.W_edge = nn.Linear(128 * 2, 256)\n",
    "      self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      \"\"\"\n",
    "      Reset parameters whenever object created\n",
    "      \"\"\"\n",
    "      gain = nn.init.calculate_gain('relu')\n",
    "      nn.init.xavier_uniform_(self.W_apply.weight, gain=gain)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "      \"\"\"\n",
    "      It sends the 'h' feature data from edges to nodes\n",
    "      \"\"\"\n",
    "      return {'m':  edges.data['h']}\n",
    "\n",
    "    def forward(self, g_dgl, nfeats, efeats):\n",
    "      \"\"\"\n",
    "      update_all : message aggregation\n",
    "      applies a linear transformation\n",
    "      concatenates node features with aggregated neighbor features\n",
    "      then applies a non-linear activation function (ReLU)\n",
    "      \"\"\"\n",
    "      with g_dgl.local_scope():\n",
    "        g = g_dgl\n",
    "        g.ndata['h'] = nfeats\n",
    "        g.edata['h'] = efeats\n",
    "        g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
    "        g.ndata['h'] = F.relu(self.W_apply(torch.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
    "\n",
    "        # Compute edge embeddings\n",
    "        u, v = g.edges()\n",
    "        edge = self.W_edge(torch.cat((g.srcdata['h'][u], g.dstdata['h'][v]), 2))\n",
    "        return g.ndata['h'], edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "_xo-3K4QRGqc"
   },
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim,  activation):\n",
    "      super(SAGE, self).__init__()\n",
    "      self.layers = nn.ModuleList()\n",
    "      self.layers.append(SAGELayer(ndim_in, edim, 128, F.relu))\n",
    "\n",
    "    def forward(self, g, nfeats, efeats, corrupt=False):\n",
    "      \"\"\"\n",
    "      If corruption : permutate edge features\n",
    "      Then send data into layers to find node&edge features\n",
    "      \"\"\"\n",
    "      if corrupt:\n",
    "        e_perm = torch.randperm(g.number_of_edges())\n",
    "        efeats = efeats[e_perm]\n",
    "      for i, layer in enumerate(self.layers):\n",
    "        nfeats, e_feats = layer(g, nfeats, efeats)\n",
    "      return nfeats.sum(1), e_feats.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5r1h02I32yd"
   },
   "source": [
    "# Self-Supervised Learning\n",
    "### Deep Graph Infomax (DGI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def uniform(self, size, tensor):\n",
    "        bound = 1.0 / math.sqrt(size)\n",
    "        if tensor is not None:\n",
    "            tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.weight.size(0)\n",
    "        self.uniform(size, self.weight)\n",
    "\n",
    "    def forward(self, features, summary):\n",
    "        features = torch.matmul(features, torch.matmul(self.weight, summary))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGI(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation):\n",
    "        super(DGI, self).__init__()\n",
    "        self.encoder = SAGE(ndim_in, ndim_out, edim,  F.relu)\n",
    "        #self.discriminator = Discriminator(128)\n",
    "        self.discriminator = Discriminator(256)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # def forward(self, graph, feat, e_features, edge_weight=None, embed=False):\n",
    "    #     feat = torch.reshape(train_g.ndata['h'],\n",
    "    #                                (train_g.ndata['h'].shape[0], 1,\n",
    "    #                                 39))\n",
    "    #     positive = self.encoder(graph, feat, e_features, corrupt=False)\n",
    "    #     negative = self.encoder(graph, feat, e_features, corrupt=True)\n",
    "    #     self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, graph, feat, e_features, edge_weight=None, eweight=None, embed=False):\n",
    " \n",
    "        feat = torch.reshape(train_g.ndata['h'],\n",
    "                                   (train_g.ndata['h'].shape[0], 1,\n",
    "                                    39))\n",
    "        positive = self.encoder(graph, feat, e_features, corrupt=False)\n",
    "        negative = self.encoder(graph, feat, e_features, corrupt=True)\n",
    "        if embed:\n",
    "            return e_features\n",
    "            #return feat\n",
    "\n",
    "        positive = positive[1]\n",
    "        negative = negative[1]\n",
    "\n",
    "        summary = torch.sigmoid(positive.mean(dim=0))\n",
    "\n",
    "        positive = self.discriminator(positive, summary)\n",
    "        negative = self.discriminator(negative, summary)\n",
    "\n",
    "        l1 = self.loss(positive, torch.ones_like(positive))\n",
    "        l2 = self.loss(negative, torch.zeros_like(negative))\n",
    "\n",
    "        return torch.tensor([[l1+l2, 0]],requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DGI\n",
    "* Same hyperparameters and optimizer specified in the \"Anomal-E\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "sKnfpWFMR19u"
   },
   "outputs": [],
   "source": [
    "ndim_in = train_g.ndata['h'].shape[1]\n",
    "hidden_features = 128\n",
    "ndim_out = 128\n",
    "num_layers = 1\n",
    "edim = train_g.edata['h'].shape[1]\n",
    "learning_rate = 1e-3\n",
    "epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgi = DGI(ndim_in,\n",
    "    ndim_out,\n",
    "    edim,\n",
    "    F.relu)\n",
    "\n",
    "dgi_optimizer = torch.optim.Adam(dgi.parameters(),\n",
    "                lr=1e-3,\n",
    "                weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "9K6_cOiWSdJA"
   },
   "outputs": [],
   "source": [
    "# Format node and edge features for E-GraphSAGE\n",
    "train_g.ndata['h'] = torch.reshape(train_g.ndata['h'],\n",
    "                                   (train_g.ndata['h'].shape[0], 1,\n",
    "                                    train_g.ndata['h'].shape[1]))\n",
    "\n",
    "train_g.edata['h'] = torch.reshape(train_g.edata['h'],\n",
    "                                   (train_g.edata['h'].shape[0], 1,\n",
    "                                    train_g.edata['h'].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "O44auIyWSexg"
   },
   "outputs": [],
   "source": [
    "# Convert to GPU\n",
    "train_g = train_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = train_g.ndata['h']\n",
    "edge_features = train_g.edata['h']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcsnJviS32ye"
   },
   "source": [
    "## Loading trained DGI\n",
    "* Same hyperparameters and optimizer specified in the \"Anomal-E\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZ2HAQDAF-4c",
    "outputId": "14699e3d-3b8f-4430-c154-9c7b35c8bd33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgi.load_state_dict(torch.load('best_dgi.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.9682, 0.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgi(train_g, node_features, edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdIYqHjg7yU9",
    "outputId": "9098170d-4f0c-4cd5-adb8-b27ae39e7ac6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=64237, num_edges=2641554,\n",
       "      ndata_schemes={'h': Scheme(shape=(1, 39), dtype=torch.float32)}\n",
       "      edata_schemes={'Label': Scheme(shape=(), dtype=torch.int64), 'Attack': Scheme(shape=(), dtype=torch.int64), 'h': Scheme(shape=(1, 39), dtype=torch.float32)})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKANXc2-32ye"
   },
   "source": [
    "## Edge Embeddings\n",
    "\n",
    "\n",
    "* Training DGI \n",
    "* Seperate encoders are trained for training and testing graph\n",
    "* After encoding train and test graphs, encodings are converted into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "6Ek16GkRStKP"
   },
   "outputs": [],
   "source": [
    "training_emb = dgi.encoder(train_g, train_g.ndata['h'], train_g.edata['h'])[1]\n",
    "training_emb = training_emb.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "-FwaBlOdS4ep"
   },
   "outputs": [],
   "source": [
    "test_g.ndata['h'] = torch.reshape(test_g.ndata['h'],\n",
    "                                   (test_g.ndata['h'].shape[0], 1,\n",
    "                                    test_g.ndata['h'].shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "test_g.edata['h'] = torch.reshape(test_g.edata['h'],\n",
    "                                   (test_g.edata['h'].shape[0], 1,\n",
    "                                    test_g.edata['h'].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "SBa-rdivS6cQ"
   },
   "outputs": [],
   "source": [
    "# Convert to GPU\n",
    "test_g = test_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "W12WLjslS-kx"
   },
   "outputs": [],
   "source": [
    "testing_emb = dgi.encoder(test_g, test_g.ndata['h'], test_g.edata['h'])[1]\n",
    "testing_emb = testing_emb.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "ERsOAMjeS_D8"
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(training_emb, )\n",
    "df_train[\"Attack\"] = lab_enc.inverse_transform(\n",
    "        train_g.edata['Attack'].detach().cpu().numpy())\n",
    "df_train[\"Label\"] = train_g.edata['Label'].detach().cpu().numpy()\n",
    "\n",
    "df_test = pd.DataFrame(testing_emb, )\n",
    "df_test[\"Attack\"] = lab_enc.inverse_transform(\n",
    "        test_g.edata['Attack'].detach().cpu().numpy())\n",
    "df_test[\"Label\"] = test_g.edata['Label'].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "B8p79H9dat5T",
    "outputId": "3a2bd004-1a23-4f37-a38a-588a56231399"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.213101</td>\n",
       "      <td>0.084576</td>\n",
       "      <td>-0.100087</td>\n",
       "      <td>-0.085792</td>\n",
       "      <td>-0.504479</td>\n",
       "      <td>-0.286598</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>-0.109970</td>\n",
       "      <td>-0.071820</td>\n",
       "      <td>-0.931622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443378</td>\n",
       "      <td>-0.176202</td>\n",
       "      <td>-0.041914</td>\n",
       "      <td>-0.069238</td>\n",
       "      <td>0.232846</td>\n",
       "      <td>-0.652440</td>\n",
       "      <td>-0.520690</td>\n",
       "      <td>-0.528282</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.213101</td>\n",
       "      <td>0.084576</td>\n",
       "      <td>-0.100087</td>\n",
       "      <td>-0.085792</td>\n",
       "      <td>-0.504479</td>\n",
       "      <td>-0.286598</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>-0.109970</td>\n",
       "      <td>-0.071820</td>\n",
       "      <td>-0.931622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443378</td>\n",
       "      <td>-0.176202</td>\n",
       "      <td>-0.041914</td>\n",
       "      <td>-0.069238</td>\n",
       "      <td>0.232846</td>\n",
       "      <td>-0.652440</td>\n",
       "      <td>-0.520690</td>\n",
       "      <td>-0.528282</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.213101</td>\n",
       "      <td>0.084576</td>\n",
       "      <td>-0.100087</td>\n",
       "      <td>-0.085792</td>\n",
       "      <td>-0.504479</td>\n",
       "      <td>-0.286598</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>-0.109970</td>\n",
       "      <td>-0.071820</td>\n",
       "      <td>-0.931622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443378</td>\n",
       "      <td>-0.176202</td>\n",
       "      <td>-0.041914</td>\n",
       "      <td>-0.069238</td>\n",
       "      <td>0.232846</td>\n",
       "      <td>-0.652440</td>\n",
       "      <td>-0.520690</td>\n",
       "      <td>-0.528282</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.213101</td>\n",
       "      <td>0.084576</td>\n",
       "      <td>-0.100087</td>\n",
       "      <td>-0.085792</td>\n",
       "      <td>-0.504479</td>\n",
       "      <td>-0.286598</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>-0.109970</td>\n",
       "      <td>-0.071820</td>\n",
       "      <td>-0.931622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443378</td>\n",
       "      <td>-0.176202</td>\n",
       "      <td>-0.041914</td>\n",
       "      <td>-0.069238</td>\n",
       "      <td>0.232846</td>\n",
       "      <td>-0.652440</td>\n",
       "      <td>-0.520690</td>\n",
       "      <td>-0.528282</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.213101</td>\n",
       "      <td>0.084576</td>\n",
       "      <td>-0.100087</td>\n",
       "      <td>-0.085792</td>\n",
       "      <td>-0.504479</td>\n",
       "      <td>-0.286598</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>-0.109970</td>\n",
       "      <td>-0.071820</td>\n",
       "      <td>-0.931622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443378</td>\n",
       "      <td>-0.176202</td>\n",
       "      <td>-0.041914</td>\n",
       "      <td>-0.069238</td>\n",
       "      <td>0.232846</td>\n",
       "      <td>-0.652440</td>\n",
       "      <td>-0.520690</td>\n",
       "      <td>-0.528282</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641549</th>\n",
       "      <td>0.238272</td>\n",
       "      <td>0.086273</td>\n",
       "      <td>-0.082979</td>\n",
       "      <td>-0.075177</td>\n",
       "      <td>-0.517827</td>\n",
       "      <td>-0.300409</td>\n",
       "      <td>0.021976</td>\n",
       "      <td>-0.122864</td>\n",
       "      <td>-0.048900</td>\n",
       "      <td>-0.925022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459370</td>\n",
       "      <td>-0.149649</td>\n",
       "      <td>-0.044086</td>\n",
       "      <td>-0.073823</td>\n",
       "      <td>0.225230</td>\n",
       "      <td>-0.659248</td>\n",
       "      <td>-0.506145</td>\n",
       "      <td>-0.530789</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641550</th>\n",
       "      <td>0.254560</td>\n",
       "      <td>0.067924</td>\n",
       "      <td>-0.087581</td>\n",
       "      <td>-0.067362</td>\n",
       "      <td>-0.560228</td>\n",
       "      <td>-0.311366</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>-0.094462</td>\n",
       "      <td>-0.053286</td>\n",
       "      <td>-0.910474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451368</td>\n",
       "      <td>-0.077642</td>\n",
       "      <td>-0.046995</td>\n",
       "      <td>-0.065274</td>\n",
       "      <td>0.201917</td>\n",
       "      <td>-0.667047</td>\n",
       "      <td>-0.512167</td>\n",
       "      <td>-0.542341</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641551</th>\n",
       "      <td>0.254018</td>\n",
       "      <td>0.076960</td>\n",
       "      <td>-0.065283</td>\n",
       "      <td>-0.077068</td>\n",
       "      <td>-0.562148</td>\n",
       "      <td>-0.313146</td>\n",
       "      <td>-0.011261</td>\n",
       "      <td>-0.106118</td>\n",
       "      <td>-0.058118</td>\n",
       "      <td>-0.904931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464071</td>\n",
       "      <td>-0.099481</td>\n",
       "      <td>-0.034149</td>\n",
       "      <td>-0.059856</td>\n",
       "      <td>0.233140</td>\n",
       "      <td>-0.666527</td>\n",
       "      <td>-0.509573</td>\n",
       "      <td>-0.542750</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641552</th>\n",
       "      <td>0.254433</td>\n",
       "      <td>0.075125</td>\n",
       "      <td>-0.074659</td>\n",
       "      <td>-0.076764</td>\n",
       "      <td>-0.565004</td>\n",
       "      <td>-0.303394</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>-0.099647</td>\n",
       "      <td>-0.059363</td>\n",
       "      <td>-0.909203</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.454214</td>\n",
       "      <td>-0.091502</td>\n",
       "      <td>-0.042996</td>\n",
       "      <td>-0.059958</td>\n",
       "      <td>0.221726</td>\n",
       "      <td>-0.664430</td>\n",
       "      <td>-0.513076</td>\n",
       "      <td>-0.544611</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641553</th>\n",
       "      <td>0.238900</td>\n",
       "      <td>0.080159</td>\n",
       "      <td>-0.091949</td>\n",
       "      <td>-0.064170</td>\n",
       "      <td>-0.511149</td>\n",
       "      <td>-0.312265</td>\n",
       "      <td>0.027106</td>\n",
       "      <td>-0.120265</td>\n",
       "      <td>-0.041391</td>\n",
       "      <td>-0.925184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460530</td>\n",
       "      <td>-0.137596</td>\n",
       "      <td>-0.045408</td>\n",
       "      <td>-0.079252</td>\n",
       "      <td>0.208585</td>\n",
       "      <td>-0.662638</td>\n",
       "      <td>-0.503886</td>\n",
       "      <td>-0.527317</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2641554 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "0        0.213101  0.084576 -0.100087 -0.085792 -0.504479 -0.286598  0.044959   \n",
       "1        0.213101  0.084576 -0.100087 -0.085792 -0.504479 -0.286598  0.044959   \n",
       "2        0.213101  0.084576 -0.100087 -0.085792 -0.504479 -0.286598  0.044959   \n",
       "3        0.213101  0.084576 -0.100087 -0.085792 -0.504479 -0.286598  0.044959   \n",
       "4        0.213101  0.084576 -0.100087 -0.085792 -0.504479 -0.286598  0.044959   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "2641549  0.238272  0.086273 -0.082979 -0.075177 -0.517827 -0.300409  0.021976   \n",
       "2641550  0.254560  0.067924 -0.087581 -0.067362 -0.560228 -0.311366  0.002567   \n",
       "2641551  0.254018  0.076960 -0.065283 -0.077068 -0.562148 -0.313146 -0.011261   \n",
       "2641552  0.254433  0.075125 -0.074659 -0.076764 -0.565004 -0.303394 -0.004606   \n",
       "2641553  0.238900  0.080159 -0.091949 -0.064170 -0.511149 -0.312265  0.027106   \n",
       "\n",
       "                7         8         9  ...       248       249       250  \\\n",
       "0       -0.109970 -0.071820 -0.931622  ... -0.443378 -0.176202 -0.041914   \n",
       "1       -0.109970 -0.071820 -0.931622  ... -0.443378 -0.176202 -0.041914   \n",
       "2       -0.109970 -0.071820 -0.931622  ... -0.443378 -0.176202 -0.041914   \n",
       "3       -0.109970 -0.071820 -0.931622  ... -0.443378 -0.176202 -0.041914   \n",
       "4       -0.109970 -0.071820 -0.931622  ... -0.443378 -0.176202 -0.041914   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "2641549 -0.122864 -0.048900 -0.925022  ... -0.459370 -0.149649 -0.044086   \n",
       "2641550 -0.094462 -0.053286 -0.910474  ... -0.451368 -0.077642 -0.046995   \n",
       "2641551 -0.106118 -0.058118 -0.904931  ... -0.464071 -0.099481 -0.034149   \n",
       "2641552 -0.099647 -0.059363 -0.909203  ... -0.454214 -0.091502 -0.042996   \n",
       "2641553 -0.120265 -0.041391 -0.925184  ... -0.460530 -0.137596 -0.045408   \n",
       "\n",
       "              251       252       253       254       255  Attack  Label  \n",
       "0       -0.069238  0.232846 -0.652440 -0.520690 -0.528282  Benign      0  \n",
       "1       -0.069238  0.232846 -0.652440 -0.520690 -0.528282  Benign      0  \n",
       "2       -0.069238  0.232846 -0.652440 -0.520690 -0.528282  Benign      0  \n",
       "3       -0.069238  0.232846 -0.652440 -0.520690 -0.528282  Benign      0  \n",
       "4       -0.069238  0.232846 -0.652440 -0.520690 -0.528282  Benign      0  \n",
       "...           ...       ...       ...       ...       ...     ...    ...  \n",
       "2641549 -0.073823  0.225230 -0.659248 -0.506145 -0.530789  Benign      0  \n",
       "2641550 -0.065274  0.201917 -0.667047 -0.512167 -0.542341  Benign      0  \n",
       "2641551 -0.059856  0.233140 -0.666527 -0.509573 -0.542750  Benign      0  \n",
       "2641552 -0.059958  0.221726 -0.664430 -0.513076 -0.544611  Benign      0  \n",
       "2641553 -0.079252  0.208585 -0.662638 -0.503886 -0.527317  Benign      0  \n",
       "\n",
       "[2641554 rows x 258 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train # Edge features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "OZxz0AZ_lS5B",
    "outputId": "3b4de103-989a-4277-938f-4c8f7d779551"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.212190</td>\n",
       "      <td>0.084235</td>\n",
       "      <td>-0.100871</td>\n",
       "      <td>-0.085811</td>\n",
       "      <td>-0.504105</td>\n",
       "      <td>-0.286591</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>-0.109333</td>\n",
       "      <td>-0.072740</td>\n",
       "      <td>-0.931824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442794</td>\n",
       "      <td>-0.176718</td>\n",
       "      <td>-0.041757</td>\n",
       "      <td>-0.069147</td>\n",
       "      <td>0.232927</td>\n",
       "      <td>-0.652265</td>\n",
       "      <td>-0.521252</td>\n",
       "      <td>-0.528130</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.212190</td>\n",
       "      <td>0.084235</td>\n",
       "      <td>-0.100871</td>\n",
       "      <td>-0.085811</td>\n",
       "      <td>-0.504105</td>\n",
       "      <td>-0.286591</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>-0.109333</td>\n",
       "      <td>-0.072740</td>\n",
       "      <td>-0.931824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442794</td>\n",
       "      <td>-0.176718</td>\n",
       "      <td>-0.041757</td>\n",
       "      <td>-0.069147</td>\n",
       "      <td>0.232927</td>\n",
       "      <td>-0.652265</td>\n",
       "      <td>-0.521252</td>\n",
       "      <td>-0.528130</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.212190</td>\n",
       "      <td>0.084235</td>\n",
       "      <td>-0.100871</td>\n",
       "      <td>-0.085811</td>\n",
       "      <td>-0.504105</td>\n",
       "      <td>-0.286591</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>-0.109333</td>\n",
       "      <td>-0.072740</td>\n",
       "      <td>-0.931824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442794</td>\n",
       "      <td>-0.176718</td>\n",
       "      <td>-0.041757</td>\n",
       "      <td>-0.069147</td>\n",
       "      <td>0.232927</td>\n",
       "      <td>-0.652265</td>\n",
       "      <td>-0.521252</td>\n",
       "      <td>-0.528130</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.212190</td>\n",
       "      <td>0.084235</td>\n",
       "      <td>-0.100871</td>\n",
       "      <td>-0.085811</td>\n",
       "      <td>-0.504105</td>\n",
       "      <td>-0.286591</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>-0.109333</td>\n",
       "      <td>-0.072740</td>\n",
       "      <td>-0.931824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442794</td>\n",
       "      <td>-0.176718</td>\n",
       "      <td>-0.041757</td>\n",
       "      <td>-0.069147</td>\n",
       "      <td>0.232927</td>\n",
       "      <td>-0.652265</td>\n",
       "      <td>-0.521252</td>\n",
       "      <td>-0.528130</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.212190</td>\n",
       "      <td>0.084235</td>\n",
       "      <td>-0.100871</td>\n",
       "      <td>-0.085811</td>\n",
       "      <td>-0.504105</td>\n",
       "      <td>-0.286591</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>-0.109333</td>\n",
       "      <td>-0.072740</td>\n",
       "      <td>-0.931824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442794</td>\n",
       "      <td>-0.176718</td>\n",
       "      <td>-0.041757</td>\n",
       "      <td>-0.069147</td>\n",
       "      <td>0.232927</td>\n",
       "      <td>-0.652265</td>\n",
       "      <td>-0.521252</td>\n",
       "      <td>-0.528130</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132126</th>\n",
       "      <td>0.238991</td>\n",
       "      <td>0.080186</td>\n",
       "      <td>-0.089852</td>\n",
       "      <td>-0.066152</td>\n",
       "      <td>-0.511611</td>\n",
       "      <td>-0.311789</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>-0.120675</td>\n",
       "      <td>-0.042586</td>\n",
       "      <td>-0.925042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460239</td>\n",
       "      <td>-0.139137</td>\n",
       "      <td>-0.044112</td>\n",
       "      <td>-0.079347</td>\n",
       "      <td>0.211982</td>\n",
       "      <td>-0.662015</td>\n",
       "      <td>-0.504174</td>\n",
       "      <td>-0.528458</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132127</th>\n",
       "      <td>0.254403</td>\n",
       "      <td>0.075197</td>\n",
       "      <td>-0.073904</td>\n",
       "      <td>-0.075690</td>\n",
       "      <td>-0.563776</td>\n",
       "      <td>-0.306023</td>\n",
       "      <td>-0.005386</td>\n",
       "      <td>-0.100197</td>\n",
       "      <td>-0.058240</td>\n",
       "      <td>-0.908402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455886</td>\n",
       "      <td>-0.091104</td>\n",
       "      <td>-0.041516</td>\n",
       "      <td>-0.060288</td>\n",
       "      <td>0.221319</td>\n",
       "      <td>-0.665003</td>\n",
       "      <td>-0.511931</td>\n",
       "      <td>-0.543905</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132128</th>\n",
       "      <td>0.226226</td>\n",
       "      <td>0.082544</td>\n",
       "      <td>-0.094559</td>\n",
       "      <td>-0.076982</td>\n",
       "      <td>-0.505063</td>\n",
       "      <td>-0.296730</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>-0.120199</td>\n",
       "      <td>-0.053299</td>\n",
       "      <td>-0.929006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452944</td>\n",
       "      <td>-0.162573</td>\n",
       "      <td>-0.045611</td>\n",
       "      <td>-0.076051</td>\n",
       "      <td>0.224357</td>\n",
       "      <td>-0.657492</td>\n",
       "      <td>-0.512367</td>\n",
       "      <td>-0.527756</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132129</th>\n",
       "      <td>0.242786</td>\n",
       "      <td>0.084904</td>\n",
       "      <td>-0.082050</td>\n",
       "      <td>-0.070827</td>\n",
       "      <td>-0.520388</td>\n",
       "      <td>-0.306714</td>\n",
       "      <td>0.018430</td>\n",
       "      <td>-0.121979</td>\n",
       "      <td>-0.045771</td>\n",
       "      <td>-0.923596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.461895</td>\n",
       "      <td>-0.140351</td>\n",
       "      <td>-0.043430</td>\n",
       "      <td>-0.075043</td>\n",
       "      <td>0.219978</td>\n",
       "      <td>-0.661053</td>\n",
       "      <td>-0.503455</td>\n",
       "      <td>-0.531070</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132130</th>\n",
       "      <td>0.233117</td>\n",
       "      <td>0.083323</td>\n",
       "      <td>-0.091541</td>\n",
       "      <td>-0.071754</td>\n",
       "      <td>-0.509837</td>\n",
       "      <td>-0.302233</td>\n",
       "      <td>0.030526</td>\n",
       "      <td>-0.121154</td>\n",
       "      <td>-0.047644</td>\n",
       "      <td>-0.926766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.457146</td>\n",
       "      <td>-0.151370</td>\n",
       "      <td>-0.045906</td>\n",
       "      <td>-0.076155</td>\n",
       "      <td>0.218226</td>\n",
       "      <td>-0.659885</td>\n",
       "      <td>-0.507753</td>\n",
       "      <td>-0.527844</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1132131 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "0        0.212190  0.084235 -0.100871 -0.085811 -0.504105 -0.286591  0.045796   \n",
       "1        0.212190  0.084235 -0.100871 -0.085811 -0.504105 -0.286591  0.045796   \n",
       "2        0.212190  0.084235 -0.100871 -0.085811 -0.504105 -0.286591  0.045796   \n",
       "3        0.212190  0.084235 -0.100871 -0.085811 -0.504105 -0.286591  0.045796   \n",
       "4        0.212190  0.084235 -0.100871 -0.085811 -0.504105 -0.286591  0.045796   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1132126  0.238991  0.080186 -0.089852 -0.066152 -0.511611 -0.311789  0.026515   \n",
       "1132127  0.254403  0.075197 -0.073904 -0.075690 -0.563776 -0.306023 -0.005386   \n",
       "1132128  0.226226  0.082544 -0.094559 -0.076982 -0.505063 -0.296730  0.037320   \n",
       "1132129  0.242786  0.084904 -0.082050 -0.070827 -0.520388 -0.306714  0.018430   \n",
       "1132130  0.233117  0.083323 -0.091541 -0.071754 -0.509837 -0.302233  0.030526   \n",
       "\n",
       "                7         8         9  ...       248       249       250  \\\n",
       "0       -0.109333 -0.072740 -0.931824  ... -0.442794 -0.176718 -0.041757   \n",
       "1       -0.109333 -0.072740 -0.931824  ... -0.442794 -0.176718 -0.041757   \n",
       "2       -0.109333 -0.072740 -0.931824  ... -0.442794 -0.176718 -0.041757   \n",
       "3       -0.109333 -0.072740 -0.931824  ... -0.442794 -0.176718 -0.041757   \n",
       "4       -0.109333 -0.072740 -0.931824  ... -0.442794 -0.176718 -0.041757   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "1132126 -0.120675 -0.042586 -0.925042  ... -0.460239 -0.139137 -0.044112   \n",
       "1132127 -0.100197 -0.058240 -0.908402  ... -0.455886 -0.091104 -0.041516   \n",
       "1132128 -0.120199 -0.053299 -0.929006  ... -0.452944 -0.162573 -0.045611   \n",
       "1132129 -0.121979 -0.045771 -0.923596  ... -0.461895 -0.140351 -0.043430   \n",
       "1132130 -0.121154 -0.047644 -0.926766  ... -0.457146 -0.151370 -0.045906   \n",
       "\n",
       "              251       252       253       254       255  Attack  Label  \n",
       "0       -0.069147  0.232927 -0.652265 -0.521252 -0.528130  Benign      0  \n",
       "1       -0.069147  0.232927 -0.652265 -0.521252 -0.528130  Benign      0  \n",
       "2       -0.069147  0.232927 -0.652265 -0.521252 -0.528130  Benign      0  \n",
       "3       -0.069147  0.232927 -0.652265 -0.521252 -0.528130  Benign      0  \n",
       "4       -0.069147  0.232927 -0.652265 -0.521252 -0.528130  Benign      0  \n",
       "...           ...       ...       ...       ...       ...     ...    ...  \n",
       "1132126 -0.079347  0.211982 -0.662015 -0.504174 -0.528458  Benign      0  \n",
       "1132127 -0.060288  0.221319 -0.665003 -0.511931 -0.543905  Benign      0  \n",
       "1132128 -0.076051  0.224357 -0.657492 -0.512367 -0.527756  Benign      0  \n",
       "1132129 -0.075043  0.219978 -0.661053 -0.503455 -0.531070  Benign      0  \n",
       "1132130 -0.076155  0.218226 -0.659885 -0.507753 -0.527844  Benign      0  \n",
       "\n",
       "[1132131 rows x 258 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNNExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gxai = pd.DataFrame(columns = [\"GNNExplainer\",\"PGExplainer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.data import GINDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.nn import AvgPooling, GNNExplainer, PGExplainer, SubgraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the prediction for graph 0\n",
    "explainer = GNNExplainer(dgi, num_hops=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explain graph: 100%|████████████████████████████████████████████████████████████████████| 100/100 [09:07<00:00,  5.48s/it]\n"
     ]
    }
   ],
   "source": [
    "g = train_g\n",
    "features = g.ndata['h']\n",
    "edge_features = g.edata['h']\n",
    "gnn_kwargs = {\n",
    "    'e_features': edge_features,  \n",
    "}\n",
    "\n",
    "f = torch.reshape(features,(train_g.ndata['h'].shape[0], 39))\n",
    "\n",
    "\n",
    "feat_mask, edge_mask = explainer.explain_graph(g, f, **gnn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=64237, num_edges=2641554,\n",
       "      ndata_schemes={'h': Scheme(shape=(1, 39), dtype=torch.float32)}\n",
       "      edata_schemes={'Label': Scheme(shape=(), dtype=torch.int64), 'Attack': Scheme(shape=(), dtype=torch.int64), 'h': Scheme(shape=(1, 39), dtype=torch.float32)})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2770, 0.3077, 0.2718, 0.2520, 0.2886, 0.2828, 0.2772, 0.3014, 0.2459,\n",
       "        0.2853, 0.2685, 0.2600, 0.2857, 0.2692, 0.2498, 0.2726, 0.2771, 0.2444,\n",
       "        0.2447, 0.2650, 0.2609, 0.2735, 0.2820, 0.2724, 0.2987, 0.2798, 0.2646,\n",
       "        0.2961, 0.2863, 0.2457, 0.2877, 0.2747, 0.3031, 0.2793, 0.2692, 0.2736,\n",
       "        0.2414, 0.2730, 0.2847])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2767, 0.2751, 0.2768,  ..., 0.2756, 0.2758, 0.2753])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641554"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2641554])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gxai['GNNExplainer'] = edge_mask.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GNNExplainer</th>\n",
       "      <th>PGExplainer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.276744</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.275142</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.276803</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.276444</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.274406</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GNNExplainer PGExplainer\n",
       "0      0.276744         NaN\n",
       "1      0.275142         NaN\n",
       "2      0.276803         NaN\n",
       "3      0.276444         NaN\n",
       "4      0.274406         NaN"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gxai.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5556e-03, -5.9708e+00], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgi(g, features, edge_features)[0].log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64237, 1, 39])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_g.ndata['h'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 288332,  182919, 1012392,  247800, 1949184])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(edge_mask.flatten(), 5).indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = train_g\n",
    "features = g.ndata['h']\n",
    "edge_features = g.edata['h']\n",
    "gnn_kwargs = {\n",
    "    'e_features': edge_features,  \n",
    "}\n",
    "\n",
    "f = torch.reshape(features,(train_g.ndata['h'].shape[0], 39))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAGE(\n",
       "  (layers): ModuleList(\n",
       "    (0): SAGELayer(\n",
       "      (W_apply): Linear(in_features=78, out_features=128, bias=True)\n",
       "      (W_edge): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgi.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = PGExplainer(dgi, num_features=39, num_hops=2)\n",
    "\n",
    "probs, edge_weight = explainer.explain_graph(g, f, **gnn_kwargs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2507, 0.3694, 0.4155,  ..., 0.6880, 0.4402, 0.4575],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2170986"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(edge_weight.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gxai['PGExplainer'] = edge_weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GNNExplainer</th>\n",
       "      <th>PGExplainer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.275427</td>\n",
       "      <td>0.250723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.274867</td>\n",
       "      <td>0.369388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.277122</td>\n",
       "      <td>0.415487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.276152</td>\n",
       "      <td>0.379018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.274712</td>\n",
       "      <td>0.328536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641549</th>\n",
       "      <td>0.276421</td>\n",
       "      <td>0.594326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641550</th>\n",
       "      <td>0.275020</td>\n",
       "      <td>0.639202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641551</th>\n",
       "      <td>0.275678</td>\n",
       "      <td>0.688026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641552</th>\n",
       "      <td>0.274718</td>\n",
       "      <td>0.440159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641553</th>\n",
       "      <td>0.275209</td>\n",
       "      <td>0.457496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2641554 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         GNNExplainer  PGExplainer\n",
       "0            0.275427     0.250723\n",
       "1            0.274867     0.369388\n",
       "2            0.277122     0.415487\n",
       "3            0.276152     0.379018\n",
       "4            0.274712     0.328536\n",
       "...               ...          ...\n",
       "2641549      0.276421     0.594326\n",
       "2641550      0.275020     0.639202\n",
       "2641551      0.275678     0.688026\n",
       "2641552      0.274718     0.440159\n",
       "2641553      0.275209     0.457496\n",
       "\n",
       "[2641554 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gxai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gxai.to_parquet(\"graph_explainers_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubgraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = SubgraphX(dgi, num_hops=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Subgraph\n",
    "# #graph = dgl.node_subgraph(train_g, random.sample(range(0, train_g.num_nodes()+1), 100))\n",
    "\n",
    "# g = train_g\n",
    "# features = g.ndata['h']\n",
    "# edge_features = g.edata['h']\n",
    "# l = torch.tensor(0)\n",
    "\n",
    "# gnn_kwargs = {\n",
    "#     'e_features': edge_features,\n",
    "# }\n",
    "\n",
    "# f = torch.reshape(features,(g.ndata['h'].shape[0], 39))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_nodes_explain = explainer.explain_graph(g, f, target_class=l, **gnn_kwargs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
